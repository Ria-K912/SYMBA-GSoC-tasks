{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 – Physics-Informed Model for Squared Amplitude\n",
        "\n",
        "This notebook trains and evaluates a **physics-informed** encoder–decoder model that:\n",
        "\n",
        "1. **Graph-based encoding** of the Feynman diagram (vertices, external legs, propagators) via a GNN (TransformerConv). Node embeddings are concatenated with the text encoder output so the decoder cross-attends to both amplitude tokens and diagram topology.\n",
        "\n",
        "2. **Physics-type token embeddings** – each vocabulary token has a physics type (coupling, mass, Mandelstam, number, regulator, operator, imaginary, other). A learned type embedding is added to the token embedding in the decoder.\n",
        "\n",
        "**Usage:** Set `MODEL` and `DATA_DIR` in the configuration cell, then run all cells. Supports **QED** and **QCD** 2-to-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Encoding and decoding ideas**\n",
        "\n",
        "| | Idea |\n",
        "|---|------|\n",
        "| **Encoding** | **Dual encoding:** (1) **Text encoder** — standard Transformer encoder on the amplitude token sequence (with sinusoidal positional encoding). (2) **Graph encoder** — the Feynman diagram is parsed into a PyG graph (vertices, external legs, propagators); a GNN (TransformerConv layers with edge features) produces node embeddings. The two are combined into a single memory (e.g. concatenated or fused via cross-attention) so the decoder can attend to both the amplitude tokens and the diagram topology. |\n",
        "| **Decoding** | **Physics-informed decoder:** Autoregressive Transformer decoder that cross-attends to the fused memory. In addition to token embeddings, each token gets a **physics-type embedding** (coupling, mass, Mandelstam, number, regulator, operator, etc.), so the model can treat different symbol types differently. Next-token prediction with cross-entropy loss. |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and path setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "try:\n",
        "    from torch_geometric.nn import TransformerConv\n",
        "    from torch_geometric.data import Batch as PygBatch, Data as PygData\n",
        "except ImportError:\n",
        "    raise ImportError(\"torch_geometric is required: pip install torch-geometric\")\n",
        "\n",
        "\n",
        "import sys\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "from preprocess import (\n",
        "    NUM_TOKEN_TYPES,\n",
        "    Vocab,\n",
        "    load_raw_data,\n",
        "    normalize_indices,\n",
        "    tokenize_expr,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Diagram parsing to PyG graph\n",
        "\n",
        "Parse SYMBA diagram text into a graph: **nodes** = vertices ∪ external legs; **edges** = propagators (between vertices) and attachments (vertex–external)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "PARTICLE_LIST = [\"e\", \"mu\", \"tau\", \"u\", \"d\", \"s\", \"c\", \"b\", \"t\", \"A\", \"G\"]\n",
        "_PARTICLE_TO_IDX = {p: i for i, p in enumerate(PARTICLE_LIST)}\n",
        "NUM_PARTICLE_TYPES = len(PARTICLE_LIST) + 1\n",
        "\n",
        "_VERTEX_SECTION_RE = re.compile(r\"Vertex\\s+V_(\\d+):(.*?)(?=Vertex\\s+V_|\\Z)\", re.S)\n",
        "_ENTRY_RE = re.compile(\n",
        "    r\"(?P<wrappers>(?:(?:AntiPart|OffShell)\\s+)*)\"\n",
        "    r\"(?P<particle>[A-Za-z]+)\"\n",
        "    r\"\\((?P<loc>[^)]+)\\)\"\n",
        ")\n",
        "\n",
        "\n",
        "# Parse diagram text into vertices, externals, offshell propagators.\n",
        "def _parse_diagram(text: str):\n",
        "    vertices = {}\n",
        "    externals = {}\n",
        "    offshell_by_type = {}\n",
        "    for m in _VERTEX_SECTION_RE.finditer(text):\n",
        "        vid = int(m.group(1))\n",
        "        body = m.group(2)\n",
        "        entries = []\n",
        "        for em in _ENTRY_RE.finditer(body):\n",
        "            wrappers = em.group(\"wrappers\").split()\n",
        "            particle = em.group(\"particle\")\n",
        "            loc = em.group(\"loc\").strip()\n",
        "            is_anti = \"AntiPart\" in wrappers\n",
        "            is_offshell = \"OffShell\" in wrappers\n",
        "            entry = dict(particle=particle, loc=loc, is_anti=is_anti, vid=vid)\n",
        "            entries.append(entry)\n",
        "            if loc.startswith(\"X_\"):\n",
        "                externals[loc] = dict(particle=particle, is_anti=is_anti)\n",
        "            if is_offshell:\n",
        "                offshell_by_type.setdefault(particle, []).append(vid)\n",
        "        vertices[vid] = entries\n",
        "    return vertices, externals, offshell_by_type\n",
        "\n",
        "\n",
        "# One-hot particle type vector.\n",
        "def _particle_onehot(name: str):\n",
        "    idx = _PARTICLE_TO_IDX.get(name, len(PARTICLE_LIST))\n",
        "    vec = [0.0] * NUM_PARTICLE_TYPES\n",
        "    vec[idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "\n",
        "def diagram_to_graph(text: str) -> PygData:\n",
        "    vertices, externals, offshell_by_type = _parse_diagram(text)\n",
        "    node_ids = {}\n",
        "    node_feats = []\n",
        "    for vid in sorted(vertices):\n",
        "        node_ids[f\"V_{vid}\"] = len(node_ids)\n",
        "        node_feats.append([1.0, 0.0] + [0.0] * NUM_PARTICLE_TYPES)\n",
        "    for xkey in sorted(externals):\n",
        "        ext = externals[xkey]\n",
        "        node_ids[xkey] = len(node_ids)\n",
        "        feat = [0.0, 1.0 if ext[\"is_anti\"] else 0.0] + _particle_onehot(ext[\"particle\"])\n",
        "        node_feats.append(feat)\n",
        "    edge_src, edge_dst, edge_attr = [], [], []\n",
        "    for ptype, vids in offshell_by_type.items():\n",
        "        if len(vids) >= 2:\n",
        "            a, b = node_ids[f\"V_{vids[0]}\"], node_ids[f\"V_{vids[1]}\"]\n",
        "            prop_feat = [1.0] + _particle_onehot(ptype)\n",
        "            edge_src += [a, b]; edge_dst += [b, a]; edge_attr += [prop_feat, prop_feat]\n",
        "    for vid, entries in vertices.items():\n",
        "        v_node = node_ids[f\"V_{vid}\"]\n",
        "        for ent in entries:\n",
        "            if ent[\"loc\"].startswith(\"X_\") and ent[\"loc\"] in node_ids:\n",
        "                x_node = node_ids[ent[\"loc\"]]\n",
        "                att_feat = [0.0] + _particle_onehot(ent[\"particle\"])\n",
        "                edge_src += [v_node, x_node]; edge_dst += [x_node, v_node]; edge_attr += [att_feat, att_feat]\n",
        "    if not edge_src:\n",
        "        edge_index = torch.zeros(2, 0, dtype=torch.long)\n",
        "        edge_attr_t = torch.zeros(0, 1 + NUM_PARTICLE_TYPES)\n",
        "    else:\n",
        "        edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
        "        edge_attr_t = torch.tensor(edge_attr, dtype=torch.float)\n",
        "    x = torch.tensor(node_feats, dtype=torch.float)\n",
        "    return PygData(x=x, edge_index=edge_index, edge_attr=edge_attr_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and collate\n",
        "\n",
        "`GraphSeq2SeqDataset` yields `(graph, src_ids, tgt_ids)` per sample. `graph_collate` batches graphs with PyG and pads sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yields (graph, src_ids, tgt_ids) per sample.\n",
        "class GraphSeq2SeqDataset(Dataset):\n",
        "    def __init__(self, records, vocab: Vocab, max_len: Optional[int] = None):\n",
        "        self.samples = []\n",
        "        for rec in records:\n",
        "            graph = diagram_to_graph(rec[\"diagram\"])\n",
        "            src_toks = tokenize_expr(normalize_indices(rec[\"amplitude\"]))\n",
        "            tgt_toks = tokenize_expr(normalize_indices(rec[\"squared_amplitude\"]))\n",
        "            src_ids = [vocab.bos_id] + vocab.encode(src_toks) + [vocab.eos_id]\n",
        "            tgt_ids = [vocab.bos_id] + vocab.encode(tgt_toks) + [vocab.eos_id]\n",
        "            if max_len is not None:\n",
        "                src_ids = src_ids[:max_len]\n",
        "                tgt_ids = tgt_ids[:max_len]\n",
        "                if src_ids[-1] != vocab.eos_id:\n",
        "                    src_ids[-1] = vocab.eos_id\n",
        "                if tgt_ids[-1] != vocab.eos_id:\n",
        "                    tgt_ids[-1] = vocab.eos_id\n",
        "            self.samples.append((\n",
        "                graph,\n",
        "                torch.tensor(src_ids, dtype=torch.long),\n",
        "                torch.tensor(tgt_ids, dtype=torch.long),\n",
        "            ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "def graph_collate(batch, pad_id=0):\n",
        "    graphs, srcs, tgts = zip(*batch)\n",
        "    graph_batch = PygBatch.from_data_list(list(graphs))\n",
        "    src_pad = pad_sequence(srcs, batch_first=True, padding_value=pad_id)\n",
        "    tgt_pad = pad_sequence(tgts, batch_first=True, padding_value=pad_id)\n",
        "    return graph_batch, src_pad, tgt_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GNN encoder for diagrams\n",
        "\n",
        "Three layers of `TransformerConv` with **residual connections** and a skip projection to per-node embeddings of size `d_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GNN: 3× TransformerConv with residual connections.\n",
        "class DiagramGNN(nn.Module):\n",
        "    def __init__(self, node_in: int, edge_in: int, hidden: int, out: int):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(node_in, hidden)\n",
        "        self.conv1 = TransformerConv(hidden, hidden, edge_dim=edge_in)\n",
        "        self.conv2 = TransformerConv(hidden, hidden, edge_dim=edge_in)\n",
        "        self.conv3 = TransformerConv(hidden, out, edge_dim=edge_in)\n",
        "        self.norm0 = nn.LayerNorm(hidden)\n",
        "        self.norm1 = nn.LayerNorm(hidden)\n",
        "        self.norm2 = nn.LayerNorm(hidden)\n",
        "        self.norm3 = nn.LayerNorm(out)\n",
        "        self.skip_proj = nn.Linear(hidden, out) if hidden != out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        h = F.gelu(self.norm0(self.input_proj(x)))\n",
        "        h = h + F.gelu(self.norm1(self.conv1(h, edge_index, edge_attr)))\n",
        "        h = h + F.gelu(self.norm2(self.conv2(h, edge_index, edge_attr)))\n",
        "        out = F.gelu(self.norm3(self.conv3(h, edge_index, edge_attr)))\n",
        "        return out + self.skip_proj(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Physics-informed Transformer\n",
        "\n",
        "**PositionalEncoding** (sinusoidal) + **PhysicsTransformer**: text encoder + graph encoder to **cross-attention fused** memory (text attends to graph with gated residual); decoder with token + physics-type embeddings on **both** encoder and decoder sides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sinusoidal positional encoding.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 4096, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "\n",
        "class PhysicsTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-attention fused graph+text encoder to decoder with physics-type embeddings.\n",
        "    Key: text memory attends to graph via cross-attention + gated residual.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,\n",
        "        nhead: int = 8,\n",
        "        num_enc_layers: int = 3,\n",
        "        num_dec_layers: int = 3,\n",
        "        dim_ff: int = 1024,\n",
        "        dropout: float = 0.1,\n",
        "        pad_id: int = 0,\n",
        "        node_feat_dim: int = 14,\n",
        "        edge_feat_dim: int = 13,\n",
        "        gnn_hidden: int = 128,\n",
        "        num_token_types: int = NUM_TOKEN_TYPES,\n",
        "        type_ids: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_id = pad_id\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.type_embed = nn.Embedding(num_token_types, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.register_buffer(\"type_ids\", type_ids if type_ids is not None else torch.zeros(vocab_size, dtype=torch.long))\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.text_encoder = nn.TransformerEncoder(enc_layer, num_enc_layers)\n",
        "        self.gnn = DiagramGNN(node_feat_dim, edge_feat_dim, gnn_hidden, d_model)\n",
        "        self.gnn_proj = nn.Linear(d_model, d_model)\n",
        "        # Cross-attention fusion: text attends to graph\n",
        "        self.graph_cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
        "        self.cross_attn_ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(dim_ff, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ff_norm = nn.LayerNorm(d_model)\n",
        "        self.fusion_gate = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.Sigmoid())\n",
        "        dec_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(dec_layer, num_dec_layers)\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _embed_src(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        tok_emb = self.embed(ids) * math.sqrt(self.d_model)\n",
        "        typ_emb = self.type_embed(self.type_ids[ids])\n",
        "        return self.pos_enc(tok_emb + typ_emb)\n",
        "\n",
        "    def _embed_tgt(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        tok_emb = self.embed(ids) * math.sqrt(self.d_model)\n",
        "        typ_emb = self.type_embed(self.type_ids[ids])\n",
        "        return self.pos_enc(tok_emb + typ_emb)\n",
        "\n",
        "    @staticmethod\n",
        "    def _causal_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)\n",
        "\n",
        "    def _encode_graphs(self, graph_batch, batch_size: int, device: torch.device):\n",
        "        node_h = self.gnn(graph_batch.x.to(device), graph_batch.edge_index.to(device), graph_batch.edge_attr.to(device))\n",
        "        node_h = self.gnn_proj(node_h)\n",
        "        batch_vec = graph_batch.batch.to(device)\n",
        "        counts = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
        "        counts.scatter_add_(0, batch_vec, torch.ones_like(batch_vec))\n",
        "        max_nodes = int(counts.max().item())\n",
        "        padded = torch.zeros(batch_size, max_nodes, self.d_model, device=device)\n",
        "        mask = torch.ones(batch_size, max_nodes, dtype=torch.bool, device=device)\n",
        "        for b in range(batch_size):\n",
        "            sel = (batch_vec == b)\n",
        "            n = int(sel.sum().item())\n",
        "            padded[b, :n] = node_h[sel]\n",
        "            mask[b, :n] = False\n",
        "        return padded, mask\n",
        "\n",
        "    def _fuse_memory(self, text_mem, graph_mem, graph_pad_mask):\n",
        "        \"\"\"Cross-attention fusion with gated residual.\"\"\"\n",
        "        attn_out, _ = self.graph_cross_attn(text_mem, graph_mem, graph_mem, key_padding_mask=graph_pad_mask)\n",
        "        fused = self.cross_attn_norm(text_mem + attn_out)\n",
        "        fused = self.ff_norm(fused + self.cross_attn_ff(fused))\n",
        "        gate = self.fusion_gate(torch.cat([text_mem, fused], dim=-1))\n",
        "        return text_mem + gate * (fused - text_mem)\n",
        "\n",
        "    def forward(self, graph_batch, src, tgt_in):\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        tgt_pad = tgt_in == self.pad_id\n",
        "        tgt_mask = self._causal_mask(tgt_in.size(1), device)\n",
        "        text_mem = self.text_encoder(self._embed_src(src), src_key_padding_mask=src_pad)\n",
        "        graph_mem, graph_pad_mask = self._encode_graphs(graph_batch, B, device)\n",
        "        memory = self._fuse_memory(text_mem, graph_mem, graph_pad_mask)\n",
        "        dec_out = self.decoder(self._embed_tgt(tgt_in), memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "        return self.out_proj(dec_out)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, graph_batch, src, bos_id, eos_id, max_len=400):\n",
        "        self.eval()\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        text_mem = self.text_encoder(self._embed_src(src), src_key_padding_mask=src_pad)\n",
        "        graph_mem, graph_pad_mask = self._encode_graphs(graph_batch, B, device)\n",
        "        memory = self._fuse_memory(text_mem, graph_mem, graph_pad_mask)\n",
        "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=device)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = self._causal_mask(ys.size(1), device)\n",
        "            tgt_pad = ys == self.pad_id\n",
        "            out = self.decoder(self._embed_tgt(ys), memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "            nxt = self.out_proj(out[:, -1, :]).argmax(dim=-1)\n",
        "            ys = torch.cat([ys, nxt.unsqueeze(1)], dim=1)\n",
        "            finished |= nxt == eos_id\n",
        "            if finished.all():\n",
        "                break\n",
        "        return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and evaluation helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip BOS/EOS/PAD from id list.\n",
        "def _strip(ids, bos, eos, pad):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i in (bos, pad):\n",
        "            continue\n",
        "        if i == eos:\n",
        "            break\n",
        "        out.append(i)\n",
        "    return out\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimiser, scheduler, device, pad_id, max_len=None, label_smoothing=0.0, accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss, n = 0.0, 0\n",
        "    optimiser.zero_grad()\n",
        "    for i, (graph_batch, src, tgt) in enumerate(loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        if max_len is not None:\n",
        "            src, tgt = src[:, :max_len], tgt[:, :max_len]\n",
        "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        logits = model(graph_batch, src, tgt_in)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1), ignore_index=pad_id, label_smoothing=label_smoothing) / accum_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimiser.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            optimiser.zero_grad()\n",
        "        total_loss += loss.item() * accum_steps\n",
        "        n += 1\n",
        "    return total_loss / max(n, 1)\n",
        "\n",
        "\n",
        "# Eval: sequence and token accuracy; returns metrics and sample predictions.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, vocab, max_gen=400, max_src_len=None):\n",
        "    model.eval()\n",
        "    seq_correct = tok_correct = tok_total = total = 0\n",
        "    examples = []\n",
        "    for graph_batch, src, tgt in loader:\n",
        "        src = src.to(device)\n",
        "        if max_src_len is not None:\n",
        "            src = src[:, :max_src_len]\n",
        "        preds = model.generate(graph_batch, src, vocab.bos_id, vocab.eos_id, max_gen)\n",
        "        for i in range(src.size(0)):\n",
        "            pred_ids = _strip(preds[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            true_ids = _strip(tgt[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            if pred_ids == true_ids:\n",
        "                seq_correct += 1\n",
        "            mn = min(len(pred_ids), len(true_ids))\n",
        "            tok_correct += sum(p == t for p, t in zip(pred_ids[:mn], true_ids[:mn]))\n",
        "            tok_total += max(len(pred_ids), len(true_ids))\n",
        "            total += 1\n",
        "            if len(examples) < 5:\n",
        "                examples.append({\"pred\": \" \".join(vocab.decode(pred_ids)), \"true\": \" \".join(vocab.decode(true_ids))})\n",
        "    return {\"seq_acc\": seq_correct / max(total, 1), \"tok_acc\": tok_correct / max(tok_total, 1), \"n\": total, \"examples\": examples}\n",
        "\n",
        "\n",
        "# 80-10-10 split, vocab, graph+seq datasets and loaders.\n",
        "def build_data_task3(data_dir, model_prefix, seed=42, batch_size=16, max_len=None):\n",
        "    import random\n",
        "    records = load_raw_data(data_dir, model_prefix)\n",
        "    if not records:\n",
        "        raise RuntimeError(f\"No data for {model_prefix}\")\n",
        "    random.seed(seed)\n",
        "    random.shuffle(records)\n",
        "    n = len(records)\n",
        "    n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
        "    train_recs = records[:n_train]\n",
        "    val_recs = records[n_train : n_train + n_val]\n",
        "    test_recs = records[n_train + n_val :]\n",
        "    all_toks = []\n",
        "    for rec in train_recs:\n",
        "        all_toks.append(tokenize_expr(normalize_indices(rec[\"amplitude\"])))\n",
        "        all_toks.append(tokenize_expr(normalize_indices(rec[\"squared_amplitude\"])))\n",
        "    vocab = Vocab(all_toks)\n",
        "    train_ds = GraphSeq2SeqDataset(train_recs, vocab, max_len)\n",
        "    val_ds = GraphSeq2SeqDataset(val_recs, vocab, max_len)\n",
        "    test_ds = GraphSeq2SeqDataset(test_recs, vocab, max_len)\n",
        "    collate = lambda b: graph_collate(b, vocab.pad_id)\n",
        "    kw = dict(num_workers=0, pin_memory=True, collate_fn=collate)\n",
        "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kw)\n",
        "    val_ld = DataLoader(val_ds, batch_size=batch_size, shuffle=False, **kw)\n",
        "    test_ld = DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kw)\n",
        "    print(f\"[task3] {model_prefix}:  {len(train_recs)} train / {len(val_recs)} val / {len(test_recs)} test  | vocab {len(vocab)}\")\n",
        "    return train_ld, val_ld, test_ld, vocab, test_recs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configuration\n",
        "\n",
        "Set `MODEL` to `\"QED\"` or `\"QCD\"`, and `DATA_DIR` to the folder containing the SYMBA test `.txt` files. Run from the `gsoc_tasks` directory so `preprocess` can be imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"QED\"  \n",
        "DATA_DIR = \"SYMBA - Test Data\"\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = None  \n",
        "LR = 3e-4\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 3\n",
        "DIM_FF = 1024\n",
        "DROPOUT = 0.1\n",
        "LABEL_SMOOTHING = 0.0\n",
        "ACCUM_STEPS = 1\n",
        "MAX_SEQ_LEN = None  \n",
        "SEED = 42\n",
        "OUT_DIR = None\n",
        "\n",
        "# Evaluate/save best checkpoint more frequently so we don't miss the peak.\n",
        "EVAL_EVERY = 5\n",
        "EVAL_FROM_EPOCH = 10\n",
        "\n",
        "if BATCH_SIZE is None:\n",
        "    BATCH_SIZE = 16 if MODEL == \"QED\" else 2\n",
        "if MAX_SEQ_LEN is None:\n",
        "    MAX_SEQ_LEN = 300 if MODEL == \"QED\" else 1500\n",
        "if OUT_DIR is None:\n",
        "    OUT_DIR = f\"results_task3_{MODEL}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Reproducibility (helps when chasing 100% seq accuracy)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Build data and create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[task3] QED:  288 train / 36 val / 36 test  | vocab 181\n",
            "Model parameters: 6,918,709\n"
          ]
        }
      ],
      "source": [
        "train_ld, val_ld, test_ld, vocab, _ = build_data_task3(DATA_DIR, MODEL, SEED, BATCH_SIZE, MAX_SEQ_LEN)\n",
        "\n",
        "node_feat_dim = 2 + NUM_PARTICLE_TYPES\n",
        "edge_feat_dim = 1 + NUM_PARTICLE_TYPES\n",
        "\n",
        "model = PhysicsTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=D_MODEL,\n",
        "    nhead=NHEAD,\n",
        "    num_enc_layers=NUM_LAYERS,\n",
        "    num_dec_layers=NUM_LAYERS,\n",
        "    dim_ff=DIM_FF,\n",
        "    dropout=DROPOUT,\n",
        "    pad_id=vocab.pad_id,\n",
        "    node_feat_dim=node_feat_dim,\n",
        "    edge_feat_dim=edge_feat_dim,\n",
        "    gnn_hidden=128,\n",
        "    type_ids=vocab.type_ids,\n",
        ").to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model parameters: {n_params:,}\")\n",
        "\n",
        "optimiser = AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / ACCUM_STEPS)\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "scheduler = OneCycleLR(optimiser, max_lr=LR, total_steps=total_steps, pct_start=0.05)\n",
        "\n",
        "best_val_seq = -1.0\n",
        "best_val_tok = -1.0\n",
        "best_path = os.path.join(OUT_DIR, \"best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training QED (physics-informed) for 200 epochs …\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ria-khatoniar/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1336: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  key_padding_mask = F._canonical_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    1 | loss 4.7985 | 2s\n",
            "Epoch    5 | loss 1.4683 | 9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ria-khatoniar/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   10 | loss 0.4976 | val seq_acc 0.0000 | val tok_acc 0.1573 | 25s\n",
            "Epoch   15 | loss 0.1944 | val seq_acc 0.0556 | val tok_acc 0.6748 | 35s\n",
            "Epoch   20 | loss 0.0893 | val seq_acc 0.3056 | val tok_acc 0.8500 | 44s\n",
            "Epoch   25 | loss 0.0536 | val seq_acc 0.3056 | val tok_acc 0.8022 | 53s\n",
            "Epoch   30 | loss 0.0345 | val seq_acc 0.6389 | val tok_acc 0.8785 | 63s\n",
            "Epoch   35 | loss 0.0277 | val seq_acc 0.4167 | val tok_acc 0.9182 | 72s\n",
            "Epoch   40 | loss 0.0180 | val seq_acc 0.8056 | val tok_acc 0.9802 | 82s\n",
            "Epoch   45 | loss 0.0141 | val seq_acc 0.7222 | val tok_acc 0.9652 | 92s\n",
            "Epoch   50 | loss 0.0127 | val seq_acc 0.8333 | val tok_acc 0.9849 | 102s\n",
            "Epoch   55 | loss 0.0098 | val seq_acc 0.7778 | val tok_acc 0.9629 | 113s\n",
            "Epoch   60 | loss 0.0077 | val seq_acc 0.8611 | val tok_acc 0.9842 | 123s\n",
            "Epoch   65 | loss 0.0090 | val seq_acc 0.4722 | val tok_acc 0.9314 | 134s\n",
            "Epoch   70 | loss 0.0066 | val seq_acc 0.4722 | val tok_acc 0.9424 | 144s\n",
            "Epoch   75 | loss 0.0043 | val seq_acc 0.8611 | val tok_acc 0.9744 | 154s\n",
            "Epoch   80 | loss 0.0042 | val seq_acc 0.9444 | val tok_acc 0.9967 | 164s\n",
            "Epoch   85 | loss 0.0041 | val seq_acc 0.9722 | val tok_acc 0.9974 | 175s\n",
            "Epoch   90 | loss 0.0059 | val seq_acc 0.7778 | val tok_acc 0.9802 | 185s\n",
            "Epoch   95 | loss 0.0056 | val seq_acc 0.9167 | val tok_acc 0.9762 | 195s\n",
            "Epoch  100 | loss 0.0049 | val seq_acc 0.9167 | val tok_acc 0.9857 | 210s\n",
            "Epoch  105 | loss 0.0032 | val seq_acc 0.9167 | val tok_acc 0.9857 | 234s\n",
            "Epoch  110 | loss 0.0028 | val seq_acc 0.8889 | val tok_acc 0.9853 | 258s\n",
            "Epoch  115 | loss 0.0029 | val seq_acc 0.9444 | val tok_acc 0.9733 | 283s\n",
            "Epoch  120 | loss 0.0014 | val seq_acc 0.9444 | val tok_acc 0.9860 | 298s\n",
            "Epoch  125 | loss 0.0014 | val seq_acc 0.9167 | val tok_acc 0.9853 | 307s\n",
            "Epoch  130 | loss 0.0016 | val seq_acc 0.8889 | val tok_acc 0.9765 | 317s\n",
            "Epoch  135 | loss 0.0014 | val seq_acc 0.9167 | val tok_acc 0.9853 | 327s\n",
            "Epoch  140 | loss 0.0012 | val seq_acc 0.9167 | val tok_acc 0.9853 | 338s\n",
            "Epoch  145 | loss 0.0011 | val seq_acc 0.9167 | val tok_acc 0.9853 | 348s\n",
            "Epoch  150 | loss 0.0011 | val seq_acc 0.8889 | val tok_acc 0.9853 | 358s\n",
            "Epoch  155 | loss 0.0008 | val seq_acc 0.8889 | val tok_acc 0.9853 | 369s\n",
            "Epoch  160 | loss 0.0006 | val seq_acc 0.8889 | val tok_acc 0.9849 | 381s\n",
            "Epoch  165 | loss 0.0010 | val seq_acc 0.8889 | val tok_acc 0.9849 | 405s\n",
            "Epoch  170 | loss 0.0007 | val seq_acc 0.8889 | val tok_acc 0.9849 | 429s\n",
            "Epoch  175 | loss 0.0008 | val seq_acc 0.9167 | val tok_acc 0.9853 | 453s\n",
            "Epoch  180 | loss 0.0004 | val seq_acc 0.9167 | val tok_acc 0.9853 | 476s\n",
            "Epoch  185 | loss 0.0005 | val seq_acc 0.9167 | val tok_acc 0.9857 | 485s\n",
            "Epoch  190 | loss 0.0007 | val seq_acc 0.9167 | val tok_acc 0.9857 | 495s\n",
            "Epoch  195 | loss 0.0007 | val seq_acc 0.9167 | val tok_acc 0.9857 | 505s\n",
            "Epoch  200 | loss 0.0008 | val seq_acc 0.9167 | val tok_acc 0.9857 | 516s\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nTraining {MODEL} (physics-informed) for {EPOCHS} epochs …\\n\")\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(\n",
        "        model, train_ld, optimiser, scheduler, device,\n",
        "        vocab.pad_id, MAX_SEQ_LEN,\n",
        "        label_smoothing=LABEL_SMOOTHING,\n",
        "        accum_steps=ACCUM_STEPS,\n",
        "    )\n",
        "\n",
        "    do_eval = (epoch >= EVAL_FROM_EPOCH) and (epoch % EVAL_EVERY == 0 or epoch == EPOCHS)\n",
        "    if do_eval:\n",
        "        metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "        elapsed = time.time() - t0\n",
        "        print(\n",
        "            f\"Epoch {epoch:4d} | loss {loss:.4f} | \"\n",
        "            f\"val seq_acc {metrics['seq_acc']:.4f} | val tok_acc {metrics['tok_acc']:.4f} | {elapsed:.0f}s\"\n",
        "        )\n",
        "\n",
        "        # Save best checkpoint (tie-break on token accuracy).\n",
        "        if (metrics[\"seq_acc\"] > best_val_seq) or (\n",
        "            metrics[\"seq_acc\"] == best_val_seq and metrics[\"tok_acc\"] > best_val_tok\n",
        "        ):\n",
        "            best_val_seq = metrics[\"seq_acc\"]\n",
        "            best_val_tok = metrics[\"tok_acc\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    elif epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | {time.time() - t0:.0f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best checkpoint and evaluating on test set …\n",
            "\n",
            "  Test Sequence Accuracy : 1.0000\n",
            "  Test Token Accuracy    : 1.0000\n",
            "  Test examples          : 36\n",
            "\n",
            "Sample predictions:\n",
            "\n",
            "  [0] TRUE : 1/36 * e ^ 4 * ( 16 * m_s ^ 2 * m_mu ^ 2 + (-8) * m_s ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_mu ^ 2 * s_24 + 8 * s_12 *…\n",
            "      PRED : 1/36 * e ^ 4 * ( 16 * m_s ^ 2 * m_mu ^ 2 + (-8) * m_s ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_mu ^ 2 * s_24 + 8 * s_12 *…\n",
            "\n",
            "  [1] TRUE : 2/81 * e ^ 4 * s_14 * s_34 * ( s_12 + 1/2 * reg_prop ) ^ (-2) + -4/81 * i * e ^ 2 * ( i * e ^ 2 * m_b ^ 2 * ( m_b ^ 2 + …\n",
            "      PRED : 2/81 * e ^ 4 * s_14 * s_34 * ( s_12 + 1/2 * reg_prop ) ^ (-2) + -4/81 * i * e ^ 2 * ( i * e ^ 2 * m_b ^ 2 * ( m_b ^ 2 + …\n",
            "\n",
            "  [2] TRUE : 4/81 * e ^ 4 * ( 16 * m_c ^ 2 * m_u ^ 2 + (-8) * m_c ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_u ^ 2 * s_24 + 8 * s_12 * s…\n",
            "      PRED : 4/81 * e ^ 4 * ( 16 * m_c ^ 2 * m_u ^ 2 + (-8) * m_c ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_u ^ 2 * s_24 + 8 * s_12 * s…\n",
            "\n",
            "Results saved to results_task3_QED\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading best checkpoint and evaluating on test set …\")\n",
        "model.load_state_dict(torch.load(best_path, weights_only=True))\n",
        "\n",
        "test_metrics = evaluate(model, test_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "print(f\"\\n  Test Sequence Accuracy : {test_metrics['seq_acc']:.4f}\")\n",
        "print(f\"  Test Token Accuracy    : {test_metrics['tok_acc']:.4f}\")\n",
        "print(f\"  Test examples          : {test_metrics['n']}\")\n",
        "print(\"\\nSample predictions:\")\n",
        "for i, ex in enumerate(test_metrics[\"examples\"][:3]):\n",
        "    print(f\"\\n  [{i}] TRUE : {ex['true'][:120]}…\")\n",
        "    print(f\"      PRED : {ex['pred'][:120]}…\")\n",
        "print(f\"\\nResults saved to {OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
