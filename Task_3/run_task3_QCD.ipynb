{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 – Physics-Informed Model for Squared Amplitude\n",
        "\n",
        "This notebook trains and evaluates a **physics-informed** encoder–decoder model that:\n",
        "\n",
        "1. **Graph-based encoding** of the Feynman diagram (vertices, external legs, propagators) via a GNN (TransformerConv). Node embeddings are concatenated with the text encoder output so the decoder cross-attends to both amplitude tokens and diagram topology.\n",
        "\n",
        "2. **Physics-type token embeddings** – each vocabulary token has a physics type (coupling, mass, Mandelstam, number, regulator, operator, imaginary, other). A learned type embedding is added to the token embedding in the decoder.\n",
        "\n",
        "**Usage:** Set `MODEL` and `DATA_DIR` in the configuration cell, then run all cells. Supports **QED** and **QCD** 2-to-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3275"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Release PyTorch cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and path setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "try:\n",
        "    from torch_geometric.nn import TransformerConv\n",
        "    from torch_geometric.data import Batch as PygBatch, Data as PygData\n",
        "except ImportError:\n",
        "    raise ImportError(\"torch_geometric is required: pip install torch-geometric\")\n",
        "\n",
        "# Allow importing preprocess (run notebook from gsoc_tasks so preprocess.py is on path)\n",
        "import sys\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "from preprocess import (\n",
        "    NUM_TOKEN_TYPES,\n",
        "    Vocab,\n",
        "    load_raw_data,\n",
        "    normalize_indices,\n",
        "    tokenize_expr,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Diagram parsing to PyG graph\n",
        "\n",
        "Parse SYMBA diagram text into a graph: **nodes** = vertices ∪ external legs; **edges** = propagators (between vertices) and attachments (vertex–external)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "PARTICLE_LIST = [\"e\", \"mu\", \"tau\", \"u\", \"d\", \"s\", \"c\", \"b\", \"t\", \"A\", \"G\"]\n",
        "_PARTICLE_TO_IDX = {p: i for i, p in enumerate(PARTICLE_LIST)}\n",
        "NUM_PARTICLE_TYPES = len(PARTICLE_LIST) + 1\n",
        "\n",
        "_VERTEX_SECTION_RE = re.compile(r\"Vertex\\s+V_(\\d+):(.*?)(?=Vertex\\s+V_|\\Z)\", re.S)\n",
        "_ENTRY_RE = re.compile(\n",
        "    r\"(?P<wrappers>(?:(?:AntiPart|OffShell)\\s+)*)\"\n",
        "    r\"(?P<particle>[A-Za-z]+)\"\n",
        "    r\"\\((?P<loc>[^)]+)\\)\"\n",
        ")\n",
        "\n",
        "\n",
        "# Parse diagram text into vertices, externals, offshell propagators.\n",
        "def _parse_diagram(text: str):\n",
        "    vertices = {}\n",
        "    externals = {}\n",
        "    offshell_by_type = {}\n",
        "    for m in _VERTEX_SECTION_RE.finditer(text):\n",
        "        vid = int(m.group(1))\n",
        "        body = m.group(2)\n",
        "        entries = []\n",
        "        for em in _ENTRY_RE.finditer(body):\n",
        "            wrappers = em.group(\"wrappers\").split()\n",
        "            particle = em.group(\"particle\")\n",
        "            loc = em.group(\"loc\").strip()\n",
        "            is_anti = \"AntiPart\" in wrappers\n",
        "            is_offshell = \"OffShell\" in wrappers\n",
        "            entry = dict(particle=particle, loc=loc, is_anti=is_anti, vid=vid)\n",
        "            entries.append(entry)\n",
        "            if loc.startswith(\"X_\"):\n",
        "                externals[loc] = dict(particle=particle, is_anti=is_anti)\n",
        "            if is_offshell:\n",
        "                offshell_by_type.setdefault(particle, []).append(vid)\n",
        "        vertices[vid] = entries\n",
        "    return vertices, externals, offshell_by_type\n",
        "\n",
        "\n",
        "# One-hot particle type vector.\n",
        "def _particle_onehot(name: str):\n",
        "    idx = _PARTICLE_TO_IDX.get(name, len(PARTICLE_LIST))\n",
        "    vec = [0.0] * NUM_PARTICLE_TYPES\n",
        "    vec[idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "\n",
        "# Build PyG graph from diagram text (nodes = vertices + externals; edges = propagators + attachments).\n",
        "def diagram_to_graph(text: str) -> PygData:\n",
        "    vertices, externals, offshell_by_type = _parse_diagram(text)\n",
        "    node_ids = {}\n",
        "    node_feats = []\n",
        "    for vid in sorted(vertices):\n",
        "        node_ids[f\"V_{vid}\"] = len(node_ids)\n",
        "        node_feats.append([1.0, 0.0] + [0.0] * NUM_PARTICLE_TYPES)\n",
        "    for xkey in sorted(externals):\n",
        "        ext = externals[xkey]\n",
        "        node_ids[xkey] = len(node_ids)\n",
        "        feat = [0.0, 1.0 if ext[\"is_anti\"] else 0.0] + _particle_onehot(ext[\"particle\"])\n",
        "        node_feats.append(feat)\n",
        "    edge_src, edge_dst, edge_attr = [], [], []\n",
        "    for ptype, vids in offshell_by_type.items():\n",
        "        if len(vids) >= 2:\n",
        "            a, b = node_ids[f\"V_{vids[0]}\"], node_ids[f\"V_{vids[1]}\"]\n",
        "            prop_feat = [1.0] + _particle_onehot(ptype)\n",
        "            edge_src += [a, b]; edge_dst += [b, a]; edge_attr += [prop_feat, prop_feat]\n",
        "    for vid, entries in vertices.items():\n",
        "        v_node = node_ids[f\"V_{vid}\"]\n",
        "        for ent in entries:\n",
        "            if ent[\"loc\"].startswith(\"X_\") and ent[\"loc\"] in node_ids:\n",
        "                x_node = node_ids[ent[\"loc\"]]\n",
        "                att_feat = [0.0] + _particle_onehot(ent[\"particle\"])\n",
        "                edge_src += [v_node, x_node]; edge_dst += [x_node, v_node]; edge_attr += [att_feat, att_feat]\n",
        "    if not edge_src:\n",
        "        edge_index = torch.zeros(2, 0, dtype=torch.long)\n",
        "        edge_attr_t = torch.zeros(0, 1 + NUM_PARTICLE_TYPES)\n",
        "    else:\n",
        "        edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
        "        edge_attr_t = torch.tensor(edge_attr, dtype=torch.float)\n",
        "    x = torch.tensor(node_feats, dtype=torch.float)\n",
        "    return PygData(x=x, edge_index=edge_index, edge_attr=edge_attr_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and collate\n",
        "\n",
        "`GraphSeq2SeqDataset` yields `(graph, src_ids, tgt_ids)` per sample. `graph_collate` batches graphs with PyG and pads sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yields (graph, src_ids, tgt_ids) per sample.\n",
        "class GraphSeq2SeqDataset(Dataset):\n",
        "    def __init__(self, records, vocab: Vocab, max_len: Optional[int] = None):\n",
        "        self.samples = []\n",
        "        for rec in records:\n",
        "            graph = diagram_to_graph(rec[\"diagram\"])\n",
        "            src_toks = tokenize_expr(normalize_indices(rec[\"amplitude\"]))\n",
        "            tgt_toks = tokenize_expr(normalize_indices(rec[\"squared_amplitude\"]))\n",
        "            src_ids = [vocab.bos_id] + vocab.encode(src_toks) + [vocab.eos_id]\n",
        "            tgt_ids = [vocab.bos_id] + vocab.encode(tgt_toks) + [vocab.eos_id]\n",
        "            if max_len is not None:\n",
        "                src_ids = src_ids[:max_len]\n",
        "                tgt_ids = tgt_ids[:max_len]\n",
        "                if src_ids[-1] != vocab.eos_id:\n",
        "                    src_ids[-1] = vocab.eos_id\n",
        "                if tgt_ids[-1] != vocab.eos_id:\n",
        "                    tgt_ids[-1] = vocab.eos_id\n",
        "            self.samples.append((\n",
        "                graph,\n",
        "                torch.tensor(src_ids, dtype=torch.long),\n",
        "                torch.tensor(tgt_ids, dtype=torch.long),\n",
        "            ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "# Batch graphs (PyG) and pad src/tgt sequences.\n",
        "def graph_collate(batch, pad_id=0):\n",
        "    graphs, srcs, tgts = zip(*batch)\n",
        "    graph_batch = PygBatch.from_data_list(list(graphs))\n",
        "    src_pad = pad_sequence(srcs, batch_first=True, padding_value=pad_id)\n",
        "    tgt_pad = pad_sequence(tgts, batch_first=True, padding_value=pad_id)\n",
        "    return graph_batch, src_pad, tgt_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GNN encoder for diagrams\n",
        "\n",
        "Three layers of `TransformerConv` with edge features to per-node embeddings of size `d_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiagramGNN(nn.Module):\n",
        "    def __init__(self, node_in: int, edge_in: int, hidden: int, out: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = TransformerConv(node_in, hidden, edge_dim=edge_in)\n",
        "        self.conv2 = TransformerConv(hidden, hidden, edge_dim=edge_in)\n",
        "        self.conv3 = TransformerConv(hidden, out, edge_dim=edge_in)\n",
        "        self.norm1 = nn.LayerNorm(hidden)\n",
        "        self.norm2 = nn.LayerNorm(hidden)\n",
        "        self.norm3 = nn.LayerNorm(out)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        h = F.gelu(self.norm1(self.conv1(x, edge_index, edge_attr)))\n",
        "        h = F.gelu(self.norm2(self.conv2(h, edge_index, edge_attr)))\n",
        "        h = F.gelu(self.norm3(self.conv3(h, edge_index, edge_attr)))\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Physics-informed Transformer\n",
        "\n",
        "**PositionalEncoding** (sinusoidal) + **PhysicsTransformer**: text encoder + graph encoder to concatenated memory; decoder with token + physics-type embeddings; linear projection to vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sinusoidal positional encoding.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 4096, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "\n",
        "class PhysicsTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,\n",
        "        nhead: int = 8,\n",
        "        num_enc_layers: int = 3,\n",
        "        num_dec_layers: int = 3,\n",
        "        dim_ff: int = 1024,\n",
        "        dropout: float = 0.1,\n",
        "        pad_id: int = 0,\n",
        "        node_feat_dim: int = 14,\n",
        "        edge_feat_dim: int = 13,\n",
        "        gnn_hidden: int = 128,\n",
        "        num_token_types: int = NUM_TOKEN_TYPES,\n",
        "        type_ids: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_id = pad_id\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.type_embed = nn.Embedding(num_token_types, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.register_buffer(\"type_ids\", type_ids if type_ids is not None else torch.zeros(vocab_size, dtype=torch.long))\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.text_encoder = nn.TransformerEncoder(enc_layer, num_enc_layers)\n",
        "        self.gnn = DiagramGNN(node_feat_dim, edge_feat_dim, gnn_hidden, d_model)\n",
        "        self.gnn_proj = nn.Linear(d_model, d_model)\n",
        "        dec_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(dec_layer, num_dec_layers)\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _embed_src(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        return self.pos_enc(self.embed(ids) * math.sqrt(self.d_model))\n",
        "\n",
        "    def _embed_tgt(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        tok_emb = self.embed(ids) * math.sqrt(self.d_model)\n",
        "        typ_emb = self.type_embed(self.type_ids[ids])\n",
        "        return self.pos_enc(tok_emb + typ_emb)\n",
        "\n",
        "    @staticmethod\n",
        "    def _causal_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)\n",
        "\n",
        "    def _encode_graphs(self, graph_batch, batch_size: int, device: torch.device):\n",
        "        node_h = self.gnn(graph_batch.x.to(device), graph_batch.edge_index.to(device), graph_batch.edge_attr.to(device))\n",
        "        node_h = self.gnn_proj(node_h)\n",
        "        batch_vec = graph_batch.batch.to(device)\n",
        "        counts = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
        "        counts.scatter_add_(0, batch_vec, torch.ones_like(batch_vec))\n",
        "        max_nodes = int(counts.max().item())\n",
        "        padded = torch.zeros(batch_size, max_nodes, self.d_model, device=device)\n",
        "        mask = torch.ones(batch_size, max_nodes, dtype=torch.bool, device=device)\n",
        "        for b in range(batch_size):\n",
        "            sel = (batch_vec == b)\n",
        "            n = int(sel.sum().item())\n",
        "            padded[b, :n] = node_h[sel]\n",
        "            mask[b, :n] = False\n",
        "        return padded, mask\n",
        "\n",
        "    def forward(self, graph_batch, src, tgt_in):\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        tgt_pad = tgt_in == self.pad_id\n",
        "        tgt_mask = self._causal_mask(tgt_in.size(1), device)\n",
        "        text_mem = self.text_encoder(self._embed_src(src), src_key_padding_mask=src_pad)\n",
        "        graph_mem, graph_pad_mask = self._encode_graphs(graph_batch, B, device)\n",
        "        memory = torch.cat([text_mem, graph_mem], dim=1)\n",
        "        mem_pad = torch.cat([src_pad, graph_pad_mask], dim=1)\n",
        "        dec_out = self.decoder(self._embed_tgt(tgt_in), memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=mem_pad)\n",
        "        return self.out_proj(dec_out)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, graph_batch, src, bos_id, eos_id, max_len=400):\n",
        "        self.eval()\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        text_mem = self.text_encoder(self._embed_src(src), src_key_padding_mask=src_pad)\n",
        "        graph_mem, graph_pad_mask = self._encode_graphs(graph_batch, B, device)\n",
        "        memory = torch.cat([text_mem, graph_mem], dim=1)\n",
        "        mem_pad = torch.cat([src_pad, graph_pad_mask], dim=1)\n",
        "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=device)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = self._causal_mask(ys.size(1), device)\n",
        "            tgt_pad = ys == self.pad_id\n",
        "            out = self.decoder(self._embed_tgt(ys), memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=mem_pad)\n",
        "            nxt = self.out_proj(out[:, -1, :]).argmax(dim=-1)\n",
        "            ys = torch.cat([ys, nxt.unsqueeze(1)], dim=1)\n",
        "            finished |= nxt == eos_id\n",
        "            if finished.all():\n",
        "                break\n",
        "        return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and evaluation helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip BOS/EOS/PAD from id list.\n",
        "def _strip(ids, bos, eos, pad):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i in (bos, pad):\n",
        "            continue\n",
        "        if i == eos:\n",
        "            break\n",
        "        out.append(i)\n",
        "    return out\n",
        "\n",
        "\n",
        "# One epoch: CE loss, optional label smoothing and grad accumulation.\n",
        "def train_one_epoch(model, loader, optimiser, scheduler, device, pad_id, max_len=None, label_smoothing=0.0, accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss, n = 0.0, 0\n",
        "    optimiser.zero_grad()\n",
        "    for i, (graph_batch, src, tgt) in enumerate(loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        if max_len is not None:\n",
        "            src, tgt = src[:, :max_len], tgt[:, :max_len]\n",
        "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        logits = model(graph_batch, src, tgt_in)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1), ignore_index=pad_id, label_smoothing=label_smoothing) / accum_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimiser.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            optimiser.zero_grad()\n",
        "        total_loss += loss.item() * accum_steps\n",
        "        n += 1\n",
        "    return total_loss / max(n, 1)\n",
        "\n",
        "\n",
        "# Eval: sequence and token accuracy; returns metrics and sample predictions.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, vocab, max_gen=400, max_src_len=None):\n",
        "    model.eval()\n",
        "    seq_correct = tok_correct = tok_total = total = 0\n",
        "    examples = []\n",
        "    for graph_batch, src, tgt in loader:\n",
        "        src = src.to(device)\n",
        "        if max_src_len is not None:\n",
        "            src = src[:, :max_src_len]\n",
        "        preds = model.generate(graph_batch, src, vocab.bos_id, vocab.eos_id, max_gen)\n",
        "        for i in range(src.size(0)):\n",
        "            pred_ids = _strip(preds[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            true_ids = _strip(tgt[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            if pred_ids == true_ids:\n",
        "                seq_correct += 1\n",
        "            mn = min(len(pred_ids), len(true_ids))\n",
        "            tok_correct += sum(p == t for p, t in zip(pred_ids[:mn], true_ids[:mn]))\n",
        "            tok_total += max(len(pred_ids), len(true_ids))\n",
        "            total += 1\n",
        "            if len(examples) < 5:\n",
        "                examples.append({\"pred\": \" \".join(vocab.decode(pred_ids)), \"true\": \" \".join(vocab.decode(true_ids))})\n",
        "    return {\"seq_acc\": seq_correct / max(total, 1), \"tok_acc\": tok_correct / max(tok_total, 1), \"n\": total, \"examples\": examples}\n",
        "\n",
        "\n",
        "# 80-10-10 split, vocab, graph+seq datasets and loaders.\n",
        "def build_data_task3(data_dir, model_prefix, seed=42, batch_size=16, max_len=None):\n",
        "    import random\n",
        "    records = load_raw_data(data_dir, model_prefix)\n",
        "    if not records:\n",
        "        raise RuntimeError(f\"No data for {model_prefix}\")\n",
        "    random.seed(seed)\n",
        "    random.shuffle(records)\n",
        "    n = len(records)\n",
        "    n_train, n_val = int(0.8 * n), int(0.1 * n)\n",
        "    train_recs = records[:n_train]\n",
        "    val_recs = records[n_train : n_train + n_val]\n",
        "    test_recs = records[n_train + n_val :]\n",
        "    all_toks = []\n",
        "    for rec in train_recs:\n",
        "        all_toks.append(tokenize_expr(normalize_indices(rec[\"amplitude\"])))\n",
        "        all_toks.append(tokenize_expr(normalize_indices(rec[\"squared_amplitude\"])))\n",
        "    vocab = Vocab(all_toks)\n",
        "    train_ds = GraphSeq2SeqDataset(train_recs, vocab, max_len)\n",
        "    val_ds = GraphSeq2SeqDataset(val_recs, vocab, max_len)\n",
        "    test_ds = GraphSeq2SeqDataset(test_recs, vocab, max_len)\n",
        "    collate = lambda b: graph_collate(b, vocab.pad_id)\n",
        "    kw = dict(num_workers=0, pin_memory=True, collate_fn=collate)\n",
        "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kw)\n",
        "    val_ld = DataLoader(val_ds, batch_size=batch_size, shuffle=False, **kw)\n",
        "    test_ld = DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kw)\n",
        "    print(f\"[task3] {model_prefix}:  {len(train_recs)} train / {len(val_recs)} val / {len(test_recs)} test  | vocab {len(vocab)}\")\n",
        "    return train_ld, val_ld, test_ld, vocab, test_recs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configuration\n",
        "\n",
        "Set `MODEL` to `\"QED\"` or `\"QCD\"`, and `DATA_DIR` to the folder containing the SYMBA test `.txt` files. Run from the `gsoc_tasks` directory so `preprocess` can be imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"QCD\"  # or \"QCD\"\n",
        "DATA_DIR = \"SYMBA - Test Data\"\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = None  # None means 16 for QED, 2 for QCD\n",
        "LR = 3e-4\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 4\n",
        "GNN_HIDDEN = 128\n",
        "DIM_FF = 1024\n",
        "DROPOUT = 0.1\n",
        "LABEL_SMOOTHING = 0.1\n",
        "ACCUM_STEPS = 2\n",
        "MAX_SEQ_LEN = None  # None means 300 (QED) or 1500 (QCD)\n",
        "SEED = 42\n",
        "OUT_DIR = None  # None means results_task3_{MODEL}\n",
        "\n",
        "if BATCH_SIZE is None:\n",
        "    BATCH_SIZE = 16 if MODEL == \"QED\" else 2\n",
        "if MAX_SEQ_LEN is None:\n",
        "    MAX_SEQ_LEN = 300 if MODEL == \"QED\" else 1500\n",
        "if OUT_DIR is None:\n",
        "    OUT_DIR = f\"results_task3_{MODEL}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Build data and create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[task3] QCD:  187 train / 23 val / 24 test  | vocab 916\n",
            "Model parameters: 8,124,308\n"
          ]
        }
      ],
      "source": [
        "train_ld, val_ld, test_ld, vocab, _ = build_data_task3(DATA_DIR, MODEL, SEED, BATCH_SIZE, MAX_SEQ_LEN)\n",
        "\n",
        "node_feat_dim = 2 + NUM_PARTICLE_TYPES\n",
        "edge_feat_dim = 1 + NUM_PARTICLE_TYPES\n",
        "\n",
        "model = PhysicsTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=D_MODEL,\n",
        "    nhead=NHEAD,\n",
        "    num_enc_layers=NUM_LAYERS,\n",
        "    num_dec_layers=NUM_LAYERS,\n",
        "    dim_ff=DIM_FF,\n",
        "    dropout=DROPOUT,\n",
        "    pad_id=vocab.pad_id,\n",
        "    node_feat_dim=node_feat_dim,\n",
        "    edge_feat_dim=edge_feat_dim,\n",
        "    gnn_hidden=GNN_HIDDEN,\n",
        "    type_ids=vocab.type_ids,\n",
        ").to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model parameters: {n_params:,}\")\n",
        "\n",
        "optimiser = AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / ACCUM_STEPS)\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "scheduler = OneCycleLR(optimiser, max_lr=LR, total_steps=total_steps, pct_start=0.05)\n",
        "best_val_acc = -1.0\n",
        "best_path = os.path.join(OUT_DIR, \"best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training QCD (physics-informed) for 400 epochs …\n",
            "\n",
            "Epoch    5 | loss 2.8077 | 67s\n",
            "Epoch   10 | loss 1.9318 | 134s\n",
            "Epoch   15 | loss 1.6294 | 203s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ria-khatoniar/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   20 | loss 1.4396 | val seq_acc 0.0000 | val tok_acc 0.2428 | 385s\n",
            "Epoch   25 | loss 1.3675 | 453s\n",
            "Epoch   30 | loss 1.3326 | 521s\n",
            "Epoch   35 | loss 1.2775 | 586s\n",
            "Epoch   40 | loss 1.2448 | val seq_acc 0.5652 | val tok_acc 0.3953 | 769s\n",
            "Epoch   45 | loss 1.2123 | 837s\n",
            "Epoch   50 | loss 1.1874 | 905s\n",
            "Epoch   55 | loss 1.1813 | 973s\n",
            "Epoch   60 | loss 1.1507 | val seq_acc 0.6957 | val tok_acc 0.4114 | 1160s\n",
            "Epoch   65 | loss 1.1592 | 1229s\n",
            "Epoch   70 | loss 1.1147 | 1300s\n",
            "Epoch   75 | loss 1.1145 | 1368s\n",
            "Epoch   80 | loss 1.0885 | val seq_acc 0.7391 | val tok_acc 0.4303 | 1514s\n",
            "Epoch   85 | loss 1.0870 | 1582s\n",
            "Epoch   90 | loss 1.0708 | 1649s\n",
            "Epoch   95 | loss 1.0612 | 1717s\n",
            "Epoch  100 | loss 1.0552 | val seq_acc 0.7391 | val tok_acc 0.4740 | 1871s\n",
            "Epoch  105 | loss 1.0564 | 1925s\n",
            "Epoch  110 | loss 1.0534 | 1978s\n",
            "Epoch  115 | loss 1.0408 | 2032s\n",
            "Epoch  120 | loss 1.0423 | val seq_acc 0.7826 | val tok_acc 0.6226 | 2158s\n",
            "Epoch  125 | loss 1.0333 | 2211s\n",
            "Epoch  130 | loss 1.0372 | 2265s\n",
            "Epoch  135 | loss 1.0311 | 2319s\n",
            "Epoch  140 | loss 1.0328 | val seq_acc 0.7826 | val tok_acc 0.4784 | 2439s\n",
            "Epoch  145 | loss 1.0292 | 2493s\n",
            "Epoch  150 | loss 1.0254 | 2547s\n",
            "Epoch  155 | loss 1.0288 | 2601s\n",
            "Epoch  160 | loss 1.0237 | val seq_acc 0.7826 | val tok_acc 0.5832 | 2725s\n",
            "Epoch  165 | loss 1.0209 | 2780s\n",
            "Epoch  170 | loss 1.0231 | 2835s\n",
            "Epoch  175 | loss 1.0184 | 2904s\n",
            "Epoch  180 | loss 1.0253 | val seq_acc 0.7826 | val tok_acc 0.4873 | 3086s\n",
            "Epoch  185 | loss 1.0230 | 3156s\n",
            "Epoch  190 | loss 1.0187 | 3224s\n",
            "Epoch  195 | loss 1.0170 | 3293s\n",
            "Epoch  200 | loss 1.0170 | val seq_acc 0.9130 | val tok_acc 0.9980 | 3431s\n",
            "Epoch  205 | loss 1.0158 | 3483s\n",
            "Epoch  210 | loss 1.0201 | 3537s\n",
            "Epoch  215 | loss 1.0177 | 3590s\n",
            "Epoch  220 | loss 1.0169 | val seq_acc 0.8696 | val tok_acc 0.8491 | 3716s\n",
            "Epoch  225 | loss 1.0157 | 3772s\n",
            "Epoch  230 | loss 1.0130 | 3828s\n",
            "Epoch  235 | loss 1.0145 | 3888s\n",
            "Epoch  240 | loss 1.0133 | val seq_acc 0.9130 | val tok_acc 0.9980 | 4019s\n",
            "Epoch  245 | loss 1.0117 | 4074s\n",
            "Epoch  250 | loss 1.0132 | 4126s\n",
            "Epoch  255 | loss 1.0119 | 4179s\n",
            "Epoch  260 | loss 1.0119 | val seq_acc 0.9130 | val tok_acc 0.9980 | 4300s\n",
            "Epoch  265 | loss 1.0114 | 4353s\n",
            "Epoch  270 | loss 1.0110 | 4406s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m t0 = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLABEL_SMOOTHING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == EPOCHS:\n\u001b[32m     12\u001b[39m         metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimiser, scheduler, device, pad_id, max_len, label_smoothing, accum_steps)\u001b[39m\n\u001b[32m     21\u001b[39m logits = model(graph_batch, src, tgt_in)\n\u001b[32m     22\u001b[39m loss = F.cross_entropy(logits.reshape(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), tgt_out.reshape(-\u001b[32m1\u001b[39m), ignore_index=pad_id, label_smoothing=label_smoothing) / accum_steps\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % accum_steps == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i + \u001b[32m1\u001b[39m) == \u001b[38;5;28mlen\u001b[39m(loader):\n\u001b[32m     25\u001b[39m     nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(f\"\\nTraining {MODEL} (physics-informed) for {EPOCHS} epochs …\\n\")\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(\n",
        "        model, train_ld, optimiser, scheduler, device,\n",
        "        vocab.pad_id, MAX_SEQ_LEN,\n",
        "        label_smoothing=LABEL_SMOOTHING,\n",
        "        accum_steps=ACCUM_STEPS,\n",
        "    )\n",
        "    if epoch % 20 == 0 or epoch == EPOCHS:\n",
        "        metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | val seq_acc {metrics['seq_acc']:.4f} | val tok_acc {metrics['tok_acc']:.4f} | {elapsed:.0f}s\")\n",
        "        if metrics[\"seq_acc\"] > best_val_acc:\n",
        "            best_val_acc = metrics[\"seq_acc\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    elif epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | {time.time() - t0:.0f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best checkpoint and evaluating on test set …\n",
            "\n",
            "  Test Sequence Accuracy : 0.8333\n",
            "  Test Token Accuracy    : 0.9488\n",
            "  Test examples          : 24\n",
            "\n",
            "Sample predictions:\n",
            "\n",
            "  [0] TRUE : -1/6 * g ^ 4 * ( m_b ^ 2 * ( m_b ^ 2 + 1/2 * s_34 ) + s_34 * ( m_b ^ 2 + 1/2 * s_34 ) ) * ( s_12 + 1/2 * reg_prop ) ^ (-…\n",
            "      PRED : -1/6 * g ^ 4 * ( m_b ^ 2 * ( m_b ^ 2 + 1/2 * s_34 ) + s_34 * ( m_b ^ 2 + 1/2 * s_34 ) ) * ( s_12 + 1/2 * reg_prop ) ^ (-…\n",
            "\n",
            "  [1] TRUE : -1/2304 * g ^ 4 * ( s_13 * ( 128 * m_t ^ 2 + (-64) * s_13 ) + (-1024) * m_t ^ 2 * ( m_t ^ 2 + -1/2 * s_13 ) + (-224) * m…\n",
            "      PRED : -1/2304 * g ^ 4 * ( s_13 * ( 128 * m_t ^ 2 + (-64) * s_13 ) + (-1024) * m_t ^ 2 * ( m_t ^ 2 + -1/2 * s_13 ) + (-224) * m…\n",
            "\n",
            "  [2] TRUE : -1/16 * g ^ 4 * ( (-16) * m_c ^ 2 * m_t ^ 2 + (-8) * m_c ^ 2 * s_12 + (-8) * s_14 * s_23 + (-8) * s_13 * s_24 + (-8) * m…\n",
            "      PRED : -1/16 * g ^ 4 * ( (-16) * m_c ^ 2 * m_t ^ 2 + (-8) * m_c ^ 2 * s_12 + (-8) * s_14 * s_23 + (-8) * s_13 * s_24 + (-8) * m…\n",
            "\n",
            "Results saved to results_task3_QCD\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading best checkpoint and evaluating on test set …\")\n",
        "model.load_state_dict(torch.load(best_path, weights_only=True))\n",
        "\n",
        "test_metrics = evaluate(model, test_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "print(f\"\\n  Test Sequence Accuracy : {test_metrics['seq_acc']:.4f}\")\n",
        "print(f\"  Test Token Accuracy    : {test_metrics['tok_acc']:.4f}\")\n",
        "print(f\"  Test examples          : {test_metrics['n']}\")\n",
        "print(\"\\nSample predictions:\")\n",
        "for i, ex in enumerate(test_metrics[\"examples\"][:3]):\n",
        "    print(f\"\\n  [{i}] TRUE : {ex['true'][:120]}…\")\n",
        "    print(f\"      PRED : {ex['pred'][:120]}…\")\n",
        "print(f\"\\nResults saved to {OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
