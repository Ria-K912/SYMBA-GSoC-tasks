{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2 â€“ Vanilla Transformer for Squared Amplitude\n",
        "\n",
        "Train a standard **encoder-decoder Transformer** (next-token prediction) to map **amplitude** token sequences to **squared-amplitude** token sequences.\n",
        "\n",
        "1. Build 80-10-10 split and shared vocabulary (via `preprocess.build_data`).\n",
        "2. Train with teacher forcing and cross-entropy loss (optional label smoothing and gradient accumulation).\n",
        "3. Evaluate with **sequence accuracy** (exact match) and **token accuracy**.\n",
        "\n",
        "Set `MODEL` and `DATA_DIR` in the config cell; run all cells. Supports **QED** and **QCD** 2-to-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "import sys\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "from preprocess import Vocab, build_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Positional encoding and Seq2Seq Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sinusoidal positional encoding.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 4096, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "\n",
        "# Encoder-decoder transformer (amplitude to squared-amplitude).\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 8, num_enc_layers: int = 3,\n",
        "                 num_dec_layers: int = 3, dim_ff: int = 1024, dropout: float = 0.1, pad_id: int = 0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_id = pad_id\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_enc_layers,\n",
        "            num_decoder_layers=num_dec_layers, dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _embed(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        return self.pos_enc(self.embed(ids) * math.sqrt(self.d_model))\n",
        "\n",
        "    @staticmethod\n",
        "    def _causal_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
        "        src_pad = src == self.pad_id\n",
        "        tgt_pad = tgt_in == self.pad_id\n",
        "        tgt_mask = self._causal_mask(tgt_in.size(1), tgt_in.device)\n",
        "        out = self.transformer(self._embed(src), self._embed(tgt_in), tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_pad, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src: torch.Tensor, bos_id: int, eos_id: int, max_len: int = 400) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        memory = self.transformer.encoder(self._embed(src), src_key_padding_mask=src_pad)\n",
        "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=device)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = self._causal_mask(ys.size(1), device)\n",
        "            tgt_pad = ys == self.pad_id\n",
        "            out = self.transformer.decoder(self._embed(ys), memory, tgt_mask=tgt_mask,\n",
        "                tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "            nxt = self.out_proj(out[:, -1, :]).argmax(dim=-1)\n",
        "            ys = torch.cat([ys, nxt.unsqueeze(1)], dim=1)\n",
        "            finished |= nxt == eos_id\n",
        "            if finished.all():\n",
        "                break\n",
        "        return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training and evaluation helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip BOS/EOS/PAD from id list.\n",
        "def _strip(ids, bos, eos, pad):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i in (bos, pad):\n",
        "            continue\n",
        "        if i == eos:\n",
        "            break\n",
        "        out.append(i)\n",
        "    return out\n",
        "\n",
        "\n",
        "# One epoch: teacher forcing, CE loss; optional label smoothing and grad accumulation.\n",
        "def train_one_epoch(model, loader, optimiser, scheduler, device, pad_id, max_len=None, label_smoothing=0.0, accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss, n_batches = 0.0, 0\n",
        "    optimiser.zero_grad()\n",
        "    for i, (src, tgt) in enumerate(loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        if max_len is not None:\n",
        "            src, tgt = src[:, :max_len], tgt[:, :max_len]\n",
        "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1), ignore_index=pad_id, label_smoothing=label_smoothing) / accum_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimiser.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            optimiser.zero_grad()\n",
        "        total_loss += loss.item() * accum_steps\n",
        "        n_batches += 1\n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "\n",
        "# Eval: sequence and token accuracy; returns metrics and sample predictions.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, vocab, max_gen=400, max_src_len=None):\n",
        "    model.eval()\n",
        "    seq_correct = tok_correct = tok_total = total = 0\n",
        "    examples = []\n",
        "    for src, tgt in loader:\n",
        "        src = src.to(device)\n",
        "        if max_src_len is not None:\n",
        "            src = src[:, :max_src_len]\n",
        "        preds = model.generate(src, vocab.bos_id, vocab.eos_id, max_gen)\n",
        "        for i in range(src.size(0)):\n",
        "            pred_ids = _strip(preds[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            true_ids = _strip(tgt[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            if pred_ids == true_ids:\n",
        "                seq_correct += 1\n",
        "            min_len = min(len(pred_ids), len(true_ids))\n",
        "            tok_correct += sum(p == t for p, t in zip(pred_ids[:min_len], true_ids[:min_len]))\n",
        "            tok_total += max(len(pred_ids), len(true_ids))\n",
        "            total += 1\n",
        "            if len(examples) < 5:\n",
        "                examples.append({\"pred\": \" \".join(vocab.decode(pred_ids)), \"true\": \" \".join(vocab.decode(true_ids))})\n",
        "    return {\"seq_acc\": seq_correct / max(total, 1), \"tok_acc\": tok_correct / max(tok_total, 1), \"n\": total, \"examples\": examples}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration\n",
        "\n",
        "Set `MODEL` to `\"QED\"` or `\"QCD\"`. Run notebook from `gsoc_tasks` so `preprocess` is importable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"QCD\"\n",
        "DATA_DIR = \"SYMBA - Test Data\"\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE = None\n",
        "LR = 2e-4\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 4\n",
        "DIM_FF = 1024\n",
        "DROPOUT = 0.1\n",
        "LABEL_SMOOTHING = 0.1\n",
        "ACCUM_STEPS = 2\n",
        "MAX_SEQ_LEN = None\n",
        "SEED = 42\n",
        "OUT_DIR = None\n",
        "\n",
        "if BATCH_SIZE is None:\n",
        "    BATCH_SIZE = 16 if MODEL == \"QED\" else 2\n",
        "if MAX_SEQ_LEN is None:\n",
        "    MAX_SEQ_LEN = 300 if MODEL == \"QED\" else 1500\n",
        "if OUT_DIR is None:\n",
        "    OUT_DIR = f\"results_task2_{MODEL}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[preprocess] QCD:  187 train / 23 val / 24 test  | vocab 916\n",
            "[preprocess] src lengths:  min=192 max=2126 avg=532\n",
            "[preprocess] tgt lengths:  min=95 max=1349 avg=357\n",
            "Model parameters: 7,843,732\n"
          ]
        }
      ],
      "source": [
        "train_ld, val_ld, test_ld, vocab, _ = build_data(DATA_DIR, MODEL, SEED, BATCH_SIZE)\n",
        "\n",
        "model = Seq2SeqTransformer(vocab_size=len(vocab), d_model=D_MODEL, nhead=NHEAD, num_enc_layers=NUM_LAYERS,\n",
        "    num_dec_layers=NUM_LAYERS, dim_ff=DIM_FF, dropout=DROPOUT, pad_id=vocab.pad_id).to(device)\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model parameters: {n_params:,}\")\n",
        "\n",
        "optimiser = AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / ACCUM_STEPS)\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "scheduler = OneCycleLR(optimiser, max_lr=LR, total_steps=total_steps, pct_start=0.05)\n",
        "best_val_acc = -1.0\n",
        "best_path = os.path.join(OUT_DIR, \"best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training QCD for 400 epochs ...\n",
            "\n",
            "Epoch    5 | loss 2.9578 | 65s\n",
            "Epoch   10 | loss 1.9349 | 129s\n",
            "Epoch   15 | loss 1.6859 | 193s\n",
            "Epoch   20 | loss 1.4880 | val seq_acc 0.0000 | val tok_acc 0.2309 | 376s\n",
            "Epoch   25 | loss 1.3979 | 439s\n",
            "Epoch   30 | loss 1.3289 | 503s\n",
            "Epoch   35 | loss 1.2851 | 568s\n",
            "Epoch   40 | loss 1.2732 | val seq_acc 0.6087 | val tok_acc 0.3843 | 747s\n",
            "Epoch   45 | loss 1.2182 | 812s\n",
            "Epoch   50 | loss 1.2010 | 876s\n",
            "Epoch   55 | loss 1.1902 | 941s\n",
            "Epoch   60 | loss 1.1546 | val seq_acc 0.6957 | val tok_acc 0.4171 | 1120s\n",
            "Epoch   65 | loss 1.1325 | 1183s\n",
            "Epoch   70 | loss 1.1174 | 1245s\n",
            "Epoch   75 | loss 1.1105 | 1309s\n",
            "Epoch   80 | loss 1.0955 | val seq_acc 0.7391 | val tok_acc 0.4277 | 1490s\n",
            "Epoch   85 | loss 1.0846 | 1556s\n",
            "Epoch   90 | loss 1.0725 | 1622s\n",
            "Epoch   95 | loss 1.0617 | 1686s\n",
            "Epoch  100 | loss 1.0685 | val seq_acc 0.7826 | val tok_acc 0.5094 | 1872s\n",
            "Epoch  105 | loss 1.0502 | 1940s\n",
            "Epoch  110 | loss 1.0547 | 2008s\n",
            "Epoch  115 | loss 1.0491 | 2068s\n",
            "Epoch  120 | loss 1.0381 | val seq_acc 0.9130 | val tok_acc 0.9991 | 2187s\n",
            "Epoch  125 | loss 1.0393 | 2237s\n",
            "Epoch  130 | loss 1.0384 | 2288s\n",
            "Epoch  135 | loss 1.0478 | 2339s\n",
            "Epoch  140 | loss 1.0348 | val seq_acc 0.6957 | val tok_acc 0.7426 | 2456s\n",
            "Epoch  145 | loss 1.0328 | 2514s\n",
            "Epoch  150 | loss 1.0292 | 2566s\n",
            "Epoch  155 | loss 1.0292 | 2618s\n",
            "Epoch  160 | loss 1.0256 | val seq_acc 0.7391 | val tok_acc 0.4697 | 2730s\n",
            "Epoch  165 | loss 1.0297 | 2781s\n",
            "Epoch  170 | loss 1.0246 | 2832s\n",
            "Epoch  175 | loss 1.0207 | 2884s\n",
            "Epoch  180 | loss 1.0199 | val seq_acc 0.9130 | val tok_acc 0.9980 | 3005s\n",
            "Epoch  185 | loss 1.0217 | 3056s\n",
            "Epoch  190 | loss 1.0185 | 3110s\n",
            "Epoch  195 | loss 1.0167 | 3165s\n",
            "Epoch  200 | loss 1.0192 | val seq_acc 0.8696 | val tok_acc 0.9015 | 3286s\n",
            "Epoch  205 | loss 1.0196 | 3337s\n",
            "Epoch  210 | loss 1.0173 | 3388s\n",
            "Epoch  215 | loss 1.0160 | 3439s\n",
            "Epoch  220 | loss 1.0156 | val seq_acc 0.8696 | val tok_acc 0.9969 | 3562s\n",
            "Epoch  225 | loss 1.0161 | 3614s\n",
            "Epoch  230 | loss 1.0146 | 3665s\n",
            "Epoch  235 | loss 1.0140 | 3718s\n",
            "Epoch  240 | loss 1.0149 | val seq_acc 0.9130 | val tok_acc 0.9980 | 3841s\n",
            "Epoch  245 | loss 1.0137 | 3892s\n",
            "Epoch  250 | loss 1.0139 | 3945s\n",
            "Epoch  255 | loss 1.0139 | 3996s\n",
            "Epoch  260 | loss 1.0130 | val seq_acc 0.9130 | val tok_acc 0.9980 | 4121s\n",
            "Epoch  265 | loss 1.0125 | 4171s\n",
            "Epoch  270 | loss 1.0118 | 4221s\n",
            "Epoch  275 | loss 1.0115 | 4271s\n",
            "Epoch  280 | loss 1.0113 | val seq_acc 0.9130 | val tok_acc 0.9980 | 4390s\n",
            "Epoch  285 | loss 1.0114 | 4442s\n",
            "Epoch  290 | loss 1.0115 | 4493s\n",
            "Epoch  295 | loss 1.0106 | 4543s\n",
            "Epoch  300 | loss 1.0106 | val seq_acc 0.9130 | val tok_acc 0.9980 | 4665s\n",
            "Epoch  305 | loss 1.0107 | 4716s\n",
            "Epoch  310 | loss 1.0107 | 4773s\n",
            "Epoch  315 | loss 1.0105 | 4823s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m t0 = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLABEL_SMOOTHING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == EPOCHS:\n\u001b[32m      8\u001b[39m         metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimiser, scheduler, device, pad_id, max_len, label_smoothing, accum_steps)\u001b[39m\n\u001b[32m     28\u001b[39m             scheduler.step()\n\u001b[32m     29\u001b[39m         optimiser.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * accum_steps\n\u001b[32m     31\u001b[39m     n_batches += \u001b[32m1\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mmax\u001b[39m(n_batches, \u001b[32m1\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(f\"\\nTraining {MODEL} for {EPOCHS} epochs ...\\n\")\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(model, train_ld, optimiser, scheduler, device, vocab.pad_id, MAX_SEQ_LEN,\n",
        "        label_smoothing=LABEL_SMOOTHING, accum_steps=ACCUM_STEPS)\n",
        "    if epoch % 20 == 0 or epoch == EPOCHS:\n",
        "        metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | val seq_acc {metrics['seq_acc']:.4f} | val tok_acc {metrics['tok_acc']:.4f} | {elapsed:.0f}s\")\n",
        "        if metrics[\"seq_acc\"] > best_val_acc:\n",
        "            best_val_acc = metrics[\"seq_acc\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    elif epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | {time.time() - t0:.0f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best checkpoint and evaluating on test set ...\n",
            "\n",
            "  Test Sequence Accuracy : 0.8750\n",
            "  Test Token Accuracy    : 0.8764\n",
            "  Test examples          : 24\n",
            "\n",
            "Sample predictions:\n",
            "\n",
            "  [0] TRUE : -1/6 * g ^ 4 * ( m_b ^ 2 * ( m_b ^ 2 + 1/2 * s_34 ) + s_34 * ( m_b ^ 2 + 1/2 * s_34 ) ) * ( s_12 + 1/2 * reg_prop ) ^ (-...\n",
            "      PRED : -1/2304 * g ^ 4 * ( s_13 * ( 128 * m_b ^ 2 + (-64) * s_13 ) + (-1024) * m_b ^ 2 * ( m_b ^ 2 + -1/2 * s_13 ) + (-224) * m...\n",
            "\n",
            "  [1] TRUE : -1/2304 * g ^ 4 * ( s_13 * ( 128 * m_t ^ 2 + (-64) * s_13 ) + (-1024) * m_t ^ 2 * ( m_t ^ 2 + -1/2 * s_13 ) + (-224) * m...\n",
            "      PRED : -1/2304 * g ^ 4 * ( s_13 * ( 128 * m_t ^ 2 + (-64) * s_13 ) + (-1024) * m_t ^ 2 * ( m_t ^ 2 + -1/2 * s_13 ) + (-224) * m...\n",
            "\n",
            "  [2] TRUE : -1/16 * g ^ 4 * ( (-16) * m_c ^ 2 * m_t ^ 2 + (-8) * m_c ^ 2 * s_12 + (-8) * s_14 * s_23 + (-8) * s_13 * s_24 + (-8) * m...\n",
            "      PRED : -1/16 * g ^ 4 * ( (-16) * m_c ^ 2 * m_t ^ 2 + (-8) * m_c ^ 2 * s_12 + (-8) * s_14 * s_23 + (-8) * s_13 * s_24 + (-8) * m...\n",
            "\n",
            "Results saved to results_task2_QCD\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading best checkpoint and evaluating on test set ...\")\n",
        "model.load_state_dict(torch.load(best_path, weights_only=True))\n",
        "\n",
        "test_metrics = evaluate(model, test_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "print(f\"\\n  Test Sequence Accuracy : {test_metrics['seq_acc']:.4f}\")\n",
        "print(f\"  Test Token Accuracy    : {test_metrics['tok_acc']:.4f}\")\n",
        "print(f\"  Test examples          : {test_metrics['n']}\")\n",
        "print(\"\\nSample predictions:\")\n",
        "for i, ex in enumerate(test_metrics[\"examples\"][:3]):\n",
        "    print(f\"\\n  [{i}] TRUE : {ex['true'][:120]}...\")\n",
        "    print(f\"      PRED : {ex['pred'][:120]}...\")\n",
        "print(f\"\\nResults saved to {OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
