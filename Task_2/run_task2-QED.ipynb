{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2 â€“ Vanilla Transformer for Squared Amplitude\n",
        "\n",
        "Train a standard **encoder-decoder Transformer** (next-token prediction) to map **amplitude** token sequences to **squared-amplitude** token sequences.\n",
        "\n",
        "1. Build 80-10-10 split and shared vocabulary (via `preprocess.build_data`).\n",
        "2. Train with teacher forcing and cross-entropy loss (optional label smoothing and gradient accumulation).\n",
        "3. Evaluate with **sequence accuracy** (exact match) and **token accuracy**.\n",
        "\n",
        "Set `MODEL` and `DATA_DIR` in the config cell; run all cells. Supports **QED** and **QCD** 2-to-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "import sys\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "from preprocess import Vocab, build_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Positional encoding and Seq2Seq Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sinusoidal positional encoding.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 4096, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "\n",
        "# Encoder-decoder transformer (amplitude to squared-amplitude).\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 8, num_enc_layers: int = 3,\n",
        "                 num_dec_layers: int = 3, dim_ff: int = 1024, dropout: float = 0.1, pad_id: int = 0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_id = pad_id\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_enc_layers,\n",
        "            num_decoder_layers=num_dec_layers, dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _embed(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        return self.pos_enc(self.embed(ids) * math.sqrt(self.d_model))\n",
        "\n",
        "    @staticmethod\n",
        "    def _causal_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
        "        src_pad = src == self.pad_id\n",
        "        tgt_pad = tgt_in == self.pad_id\n",
        "        tgt_mask = self._causal_mask(tgt_in.size(1), tgt_in.device)\n",
        "        out = self.transformer(self._embed(src), self._embed(tgt_in), tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_pad, tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src: torch.Tensor, bos_id: int, eos_id: int, max_len: int = 400) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        B, device = src.size(0), src.device\n",
        "        src_pad = src == self.pad_id\n",
        "        memory = self.transformer.encoder(self._embed(src), src_key_padding_mask=src_pad)\n",
        "        ys = torch.full((B, 1), bos_id, dtype=torch.long, device=device)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = self._causal_mask(ys.size(1), device)\n",
        "            tgt_pad = ys == self.pad_id\n",
        "            out = self.transformer.decoder(self._embed(ys), memory, tgt_mask=tgt_mask,\n",
        "                tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=src_pad)\n",
        "            nxt = self.out_proj(out[:, -1, :]).argmax(dim=-1)\n",
        "            ys = torch.cat([ys, nxt.unsqueeze(1)], dim=1)\n",
        "            finished |= nxt == eos_id\n",
        "            if finished.all():\n",
        "                break\n",
        "        return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training and evaluation helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip BOS/EOS/PAD from id list.\n",
        "def _strip(ids, bos, eos, pad):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i in (bos, pad):\n",
        "            continue\n",
        "        if i == eos:\n",
        "            break\n",
        "        out.append(i)\n",
        "    return out\n",
        "\n",
        "\n",
        "# One epoch: teacher forcing, CE loss, optional label smoothing and gradient accumulation.\n",
        "def train_one_epoch(model, loader, optimiser, scheduler, device, pad_id, max_len=None, label_smoothing=0.0, accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss, n_batches = 0.0, 0\n",
        "    optimiser.zero_grad()\n",
        "    for i, (src, tgt) in enumerate(loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        if max_len is not None:\n",
        "            src, tgt = src[:, :max_len], tgt[:, :max_len]\n",
        "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1), ignore_index=pad_id, label_smoothing=label_smoothing) / accum_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimiser.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            optimiser.zero_grad()\n",
        "        total_loss += loss.item() * accum_steps\n",
        "        n_batches += 1\n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "\n",
        "# Eval: sequence and token accuracy; returns metrics and sample predictions.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, vocab, max_gen=400, max_src_len=None):\n",
        "    model.eval()\n",
        "    seq_correct = tok_correct = tok_total = total = 0\n",
        "    examples = []\n",
        "    for src, tgt in loader:\n",
        "        src = src.to(device)\n",
        "        if max_src_len is not None:\n",
        "            src = src[:, :max_src_len]\n",
        "        preds = model.generate(src, vocab.bos_id, vocab.eos_id, max_gen)\n",
        "        for i in range(src.size(0)):\n",
        "            pred_ids = _strip(preds[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            true_ids = _strip(tgt[i].tolist(), vocab.bos_id, vocab.eos_id, vocab.pad_id)\n",
        "            if pred_ids == true_ids:\n",
        "                seq_correct += 1\n",
        "            min_len = min(len(pred_ids), len(true_ids))\n",
        "            tok_correct += sum(p == t for p, t in zip(pred_ids[:min_len], true_ids[:min_len]))\n",
        "            tok_total += max(len(pred_ids), len(true_ids))\n",
        "            total += 1\n",
        "            if len(examples) < 5:\n",
        "                examples.append({\"pred\": \" \".join(vocab.decode(pred_ids)), \"true\": \" \".join(vocab.decode(true_ids))})\n",
        "    return {\"seq_acc\": seq_correct / max(total, 1), \"tok_acc\": tok_correct / max(tok_total, 1), \"n\": total, \"examples\": examples}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration\n",
        "\n",
        "Set `MODEL` to `\"QED\"` or `\"QCD\"`. Run notebook from `gsoc_tasks` so `preprocess` is importable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"QED\"\n",
        "DATA_DIR = \"SYMBA - Test Data\"\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = None\n",
        "LR = 3e-4\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 3\n",
        "DIM_FF = 1024\n",
        "DROPOUT = 0.1\n",
        "LABEL_SMOOTHING = 0.0\n",
        "ACCUM_STEPS = 1\n",
        "MAX_SEQ_LEN = None\n",
        "SEED = 42\n",
        "OUT_DIR = None\n",
        "\n",
        "if BATCH_SIZE is None:\n",
        "    BATCH_SIZE = 16 if MODEL == \"QED\" else 2\n",
        "if MAX_SEQ_LEN is None:\n",
        "    MAX_SEQ_LEN = 300 if MODEL == \"QED\" else 1500\n",
        "if OUT_DIR is None:\n",
        "    OUT_DIR = f\"results_task2_{MODEL}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[preprocess] QED:  288 train / 36 val / 36 test  | vocab 181\n",
            "[preprocess] src lengths:  min=106 max=198 avg=141\n",
            "[preprocess] tgt lengths:  min=61 max=97 avg=75\n",
            "Model parameters: 5,623,477\n"
          ]
        }
      ],
      "source": [
        "train_ld, val_ld, test_ld, vocab, _ = build_data(DATA_DIR, MODEL, SEED, BATCH_SIZE)\n",
        "\n",
        "model = Seq2SeqTransformer(vocab_size=len(vocab), d_model=D_MODEL, nhead=NHEAD, num_enc_layers=NUM_LAYERS,\n",
        "    num_dec_layers=NUM_LAYERS, dim_ff=DIM_FF, dropout=DROPOUT, pad_id=vocab.pad_id).to(device)\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model parameters: {n_params:,}\")\n",
        "\n",
        "optimiser = AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / ACCUM_STEPS)\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "scheduler = OneCycleLR(optimiser, max_lr=LR, total_steps=total_steps, pct_start=0.05)\n",
        "best_val_acc = -1.0\n",
        "best_path = os.path.join(OUT_DIR, \"best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training QED for 200 epochs ...\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ria-khatoniar/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1336: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  key_padding_mask = F._canonical_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    5 | loss 1.5002 | 8s\n",
            "Epoch   10 | loss 0.5206 | 15s\n",
            "Epoch   15 | loss 0.1832 | 21s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ria-khatoniar/Downloads/ampgnn/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:531: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   20 | loss 0.0863 | val seq_acc 0.2778 | val tok_acc 0.8247 | 31s\n",
            "Epoch   25 | loss 0.0503 | 37s\n",
            "Epoch   30 | loss 0.0332 | 44s\n",
            "Epoch   35 | loss 0.0229 | 50s\n",
            "Epoch   40 | loss 0.0178 | val seq_acc 0.8889 | val tok_acc 0.9985 | 59s\n",
            "Epoch   45 | loss 0.0151 | 65s\n",
            "Epoch   50 | loss 0.0142 | 71s\n",
            "Epoch   55 | loss 0.0092 | 77s\n",
            "Epoch   60 | loss 0.0077 | val seq_acc 0.8333 | val tok_acc 0.9769 | 87s\n",
            "Epoch   65 | loss 0.0051 | 93s\n",
            "Epoch   70 | loss 0.0062 | 100s\n",
            "Epoch   75 | loss 0.0053 | 106s\n",
            "Epoch   80 | loss 0.0053 | val seq_acc 0.9722 | val tok_acc 0.9974 | 115s\n",
            "Epoch   85 | loss 0.0098 | 121s\n",
            "Epoch   90 | loss 0.0068 | 128s\n",
            "Epoch   95 | loss 0.0023 | 134s\n",
            "Epoch  100 | loss 0.0025 | val seq_acc 0.9722 | val tok_acc 0.9974 | 143s\n",
            "Epoch  105 | loss 0.0019 | 149s\n",
            "Epoch  110 | loss 0.0029 | 155s\n",
            "Epoch  115 | loss 0.0017 | 161s\n",
            "Epoch  120 | loss 0.0018 | val seq_acc 0.9722 | val tok_acc 0.9974 | 170s\n",
            "Epoch  125 | loss 0.0016 | 177s\n",
            "Epoch  130 | loss 0.0017 | 183s\n",
            "Epoch  135 | loss 0.0008 | 189s\n",
            "Epoch  140 | loss 0.0015 | val seq_acc 0.9722 | val tok_acc 0.9974 | 198s\n",
            "Epoch  145 | loss 0.0016 | 205s\n",
            "Epoch  150 | loss 0.0007 | 211s\n",
            "Epoch  155 | loss 0.0010 | 217s\n",
            "Epoch  160 | loss 0.0012 | val seq_acc 0.9722 | val tok_acc 0.9974 | 226s\n",
            "Epoch  165 | loss 0.0008 | 232s\n",
            "Epoch  170 | loss 0.0009 | 239s\n",
            "Epoch  175 | loss 0.0007 | 245s\n",
            "Epoch  180 | loss 0.0007 | val seq_acc 0.9722 | val tok_acc 0.9974 | 254s\n",
            "Epoch  185 | loss 0.0007 | 260s\n",
            "Epoch  190 | loss 0.0009 | 266s\n",
            "Epoch  195 | loss 0.0006 | 273s\n",
            "Epoch  200 | loss 0.0006 | val seq_acc 0.9722 | val tok_acc 0.9974 | 282s\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nTraining {MODEL} for {EPOCHS} epochs ...\\n\")\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(model, train_ld, optimiser, scheduler, device, vocab.pad_id, MAX_SEQ_LEN,\n",
        "        label_smoothing=LABEL_SMOOTHING, accum_steps=ACCUM_STEPS)\n",
        "    if epoch % 20 == 0 or epoch == EPOCHS:\n",
        "        metrics = evaluate(model, val_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | val seq_acc {metrics['seq_acc']:.4f} | val tok_acc {metrics['tok_acc']:.4f} | {elapsed:.0f}s\")\n",
        "        if metrics[\"seq_acc\"] > best_val_acc:\n",
        "            best_val_acc = metrics[\"seq_acc\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    elif epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:4d} | loss {loss:.4f} | {time.time() - t0:.0f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best checkpoint and evaluating on test set ...\n",
            "\n",
            "  Test Sequence Accuracy : 1.0000\n",
            "  Test Token Accuracy    : 1.0000\n",
            "  Test examples          : 36\n",
            "\n",
            "Sample predictions:\n",
            "\n",
            "  [0] TRUE : 1/36 * e ^ 4 * ( 16 * m_s ^ 2 * m_mu ^ 2 + (-8) * m_s ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_mu ^ 2 * s_24 + 8 * s_12 *...\n",
            "      PRED : 1/36 * e ^ 4 * ( 16 * m_s ^ 2 * m_mu ^ 2 + (-8) * m_s ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_mu ^ 2 * s_24 + 8 * s_12 *...\n",
            "\n",
            "  [1] TRUE : 2/81 * e ^ 4 * s_14 * s_34 * ( s_12 + 1/2 * reg_prop ) ^ (-2) + -4/81 * i * e ^ 2 * ( i * e ^ 2 * m_b ^ 2 * ( m_b ^ 2 + ...\n",
            "      PRED : 2/81 * e ^ 4 * s_14 * s_34 * ( s_12 + 1/2 * reg_prop ) ^ (-2) + -4/81 * i * e ^ 2 * ( i * e ^ 2 * m_b ^ 2 * ( m_b ^ 2 + ...\n",
            "\n",
            "  [2] TRUE : 4/81 * e ^ 4 * ( 16 * m_c ^ 2 * m_u ^ 2 + (-8) * m_c ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_u ^ 2 * s_24 + 8 * s_12 * s...\n",
            "      PRED : 4/81 * e ^ 4 * ( 16 * m_c ^ 2 * m_u ^ 2 + (-8) * m_c ^ 2 * s_13 + 8 * s_14 * s_23 + (-8) * m_u ^ 2 * s_24 + 8 * s_12 * s...\n",
            "\n",
            "Results saved to results_task2_QED\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading best checkpoint and evaluating on test set ...\")\n",
        "model.load_state_dict(torch.load(best_path, weights_only=True))\n",
        "\n",
        "test_metrics = evaluate(model, test_ld, device, vocab, max_gen=MAX_SEQ_LEN, max_src_len=MAX_SEQ_LEN)\n",
        "print(f\"\\n  Test Sequence Accuracy : {test_metrics['seq_acc']:.4f}\")\n",
        "print(f\"  Test Token Accuracy    : {test_metrics['tok_acc']:.4f}\")\n",
        "print(f\"  Test examples          : {test_metrics['n']}\")\n",
        "print(\"\\nSample predictions:\")\n",
        "for i, ex in enumerate(test_metrics[\"examples\"][:3]):\n",
        "    print(f\"\\n  [{i}] TRUE : {ex['true'][:120]}...\")\n",
        "    print(f\"      PRED : {ex['pred'][:120]}...\")\n",
        "print(f\"\\nResults saved to {OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
